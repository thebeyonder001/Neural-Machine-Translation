{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITNbe4E7yuaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "from pickle import load\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_9FszqrulEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical,plot_model\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W64rEcEgUlZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu as bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PCZGpDRuvGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "6f770fcc-4e59-4c48-8c2c-ecd09c128991"
      },
      "source": [
        "!wget http://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-18 12:19:21--  http://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 172.67.173.198, 104.24.109.196, 104.24.108.196, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|172.67.173.198|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4781548 (4.6M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip.1’\n",
            "\n",
            "\rspa-eng.zip.1         0%[                    ]       0  --.-KB/s               \rspa-eng.zip.1         1%[                    ]  50.41K   220KB/s               \rspa-eng.zip.1         4%[                    ] 221.50K   466KB/s               \rspa-eng.zip.1        19%[==>                 ] 903.58K  1.28MB/s               \rspa-eng.zip.1        77%[==============>     ]   3.54M  3.84MB/s               \rspa-eng.zip.1       100%[===================>]   4.56M  4.90MB/s    in 0.9s    \n",
            "\n",
            "2020-06-18 12:19:22 (4.90 MB/s) - ‘spa-eng.zip.1’ saved [4781548/4781548]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaZkjW7axxZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t')[:2] for line in  lines]\n",
        "\treturn pairs\n",
        "# clean a list of lines\n",
        "def cleaned_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2l3ftbUtp87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "758ceeae-c38d-45de-bc99-e403f735da67"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWGDka8VU5Uf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ca6b127-797e-47c6-e4dc-9562e6699dde"
      },
      "source": [
        "from shutil import copyfile\n",
        "copyfile(\"/content/drive/My Drive/en_es_data/train.txt\",\"/content/spa.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spa.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay3LEQDLysvk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "2a2e8a6d-9e01-4f42-afde-abf0ed81463f"
      },
      "source": [
        "# load dataset\n",
        "filename = 'spa.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = cleaned_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-spanish.pkl')\n",
        "# spot check\n",
        "for i in range(3):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-spanish.pkl\n",
            "[thank you so much chris and its truly a great honor to have the opportunity to come to this stage twice im extremely grateful] => [muchas gracias chris y es en verdad un gran honor tener la oportunidad de venir a este escenario por segunda vez estoy extremadamente agradecido]\n",
            "[i have been blown away by this conference and i want to thank all of you for the many nice comments about what i had to say the other night] => [he quedado conmovido por esta conferencia y deseo agradecer a todos ustedes sus amables comentarios acerca de lo que tena que decir la otra noche]\n",
            "[and i say that sincerely partly because mock sob i need that put yourselves in my position] => [y digo eso sinceramente en parte porque sollozos fingidos lo necesito pnganse en mi posicin]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq3t33iXy4Xp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2896850-4046-46b8-d0dd-e9d3395837e8"
      },
      "source": [
        "k = 0\n",
        "k1 = 0\n",
        "for i in range(0,len(clean_pairs)):\n",
        "\t# print(clean_pairs[i],len(clean_pairs[i]))\n",
        "\tif k<len(clean_pairs[i][0].split(\" \")):\n",
        "\t\tk = len(clean_pairs[i,0].split(\" \"))\n",
        "\tif k1<len(clean_pairs[i][1].split(\" \")):\n",
        "\t\tk1 = len(clean_pairs[i,1].split(\" \"))\n",
        "print(k,k1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXimLkuLz7vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "#EVALUATION FUNCTIONS\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        " \n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "  predict = model.predict(source, verbose=0)[0]\n",
        "  integers = []\n",
        "  for pred in predict:\n",
        "    integers.append(np.argmax(pred))\n",
        "  target = []\n",
        "  for i in integers:\n",
        "    word = word_for_id(i, tokenizer)\n",
        "    if word is None:\n",
        "      break\n",
        "    target.append(word)\n",
        "  return ' '.join(target)\n",
        "\n",
        "def performance(model):\n",
        "  sam = -1\n",
        "  b1=0\n",
        "  b2=0\n",
        "  b3=0\n",
        "  b4=0\n",
        "\n",
        "  min_b1=2\n",
        "  min_b2=2\n",
        "  min_b3=2\n",
        "  min_b4=2\n",
        "\n",
        "  count = 0\n",
        "  check=0\n",
        "  for j in range(0,int(len(testX)/40)):\n",
        "    count = count+1\n",
        "    sam =sam +10\n",
        "    actual=[]\n",
        "    predicted =[]\n",
        "    source = []\n",
        "    print(count)\n",
        "    x_input11=testX[sam].reshape(1,eng_length)\n",
        "    x_input21=np.zeros((1,es_length)) \n",
        "    for i in range(0,es_length):\n",
        "      output = model.predict([x_input11,x_input21])\n",
        "      out1=np.argmax(output,axis=2)\n",
        "      a = out1[0,i]\n",
        "      if i !=es_length-1:\n",
        "        x_input21[0,i+1]=a\n",
        "      if a==0:\n",
        "        break\n",
        "    prediction = x_input21[0].tolist()\n",
        "    prediction.append(a)\n",
        "\n",
        "    for w in testX[sam]:\n",
        "      try:\n",
        "        source.append(eng_tokenizer.index_word[w])\n",
        "      except:\n",
        "        source.append('')\n",
        "    for w in prediction[1:]:\n",
        "      try:\n",
        "        predicted.append(es_tokenizer.index_word[w])\n",
        "      except:\n",
        "        predicted.append('-')\n",
        "    for w in testY[sam]:\n",
        "      try:\n",
        "        actual.append(es_tokenizer.index_word[w])\n",
        "      except: \n",
        "        actual.append('-')\n",
        "    source = \" \".join(source)\n",
        "    actual = \" \".join(actual)\n",
        "    predicted = \" \".join(predicted)\n",
        "    if count<10:\n",
        "      print(\"source: %s\" % source)\n",
        "      print(\"actual: %s\" % actual)\n",
        "      print(\"predicted: %s\" % predicted)\n",
        "\n",
        "  # try:\n",
        "    che = bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
        "    if (che<min_b1):\n",
        "      min_b1 =che\n",
        "    b1=b1+ che\n",
        "\n",
        "    che = bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
        "    if (che<min_b2):\n",
        "      min_b2 =che\n",
        "    b2=b2+ che\n",
        "\n",
        "    che = bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0))\n",
        "    if (che<min_b3):\n",
        "      min_b3 =che\n",
        "    b3=b3+ che\n",
        "\n",
        "\n",
        "    che = bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    if (che<min_b4):\n",
        "      min_b4 =che\n",
        "    b4=b4+ che\n",
        "    check = check + 1\n",
        "  # except:\n",
        "    # pass\n",
        "  print(check)\n",
        "  print(b1/check,b2/check,b3/check,b4/check)\n",
        "  print(min_b1,min_b2,min_b3,min_b4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lr0WdrEXYGQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78df1d87-c56f-4d4e-812f-772e23830091"
      },
      "source": [
        "f = open(\"spa.txt\",\"r\")\n",
        "count = 0\n",
        "for line in f:\n",
        "  count = count + 1\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "216617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j898T8iHfEvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d267798b-2166-417e-864b-e36f9c0949f7"
      },
      "source": [
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-spanish.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 100000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "for i in range(0,n_sentences):\n",
        "  a = dataset[i,0].split()\n",
        "  a = \" \".join(e for e in a)\n",
        "  dataset[i,0] = a\n",
        "  a = dataset[i,1].split()\n",
        "  a = \" \".join(e for e in a)\n",
        "  dataset[i,1] = a\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "lim = int(n_sentences*0.95)\n",
        "train, test = dataset[:lim], dataset[lim:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-spanish-both.pkl')\n",
        "save_clean_data(train, 'english-spanish-train.pkl')\n",
        "save_clean_data(test, 'english-spanish-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-spanish-both.pkl\n",
            "Saved: english-spanish-train.pkl\n",
            "Saved: english-spanish-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoO4-bE2XpXS",
        "colab_type": "text"
      },
      "source": [
        "Similar operations as above but this time we omit the sentences that are too long"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej9tEa2jXo7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # load dataset\n",
        "# raw_dataset = load_clean_sentences('english-spanish.pkl')\n",
        "\n",
        "# # reduce dataset size\n",
        "# n_sentences = 100000\n",
        "# dataset = raw_dataset\n",
        "# for i in range(0,len(dataset)) and i<n_sentences:\n",
        "#   a = dataset[i,0].split()\n",
        "#   a = \" \".join(e for e in a)\n",
        "#   dataset[i,0] = a\n",
        "#   a = dataset[i,1].split()\n",
        "#   a = \" \".join(e for e in a)\n",
        "#   dataset[i,1] = a\n",
        "# # random shuffle\n",
        "# shuffle(dataset)\n",
        "# # split into train/test\n",
        "# lim = int(n_sentences*0.95)\n",
        "# train, test = dataset[:lim], dataset[lim:]\n",
        "# # save\n",
        "# save_clean_data(dataset, 'english-spanish-both.pkl')\n",
        "# save_clean_data(train, 'english-spanish-train.pkl')\n",
        "# save_clean_data(test, 'english-spanish-test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoGOh1dA1PTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = load_clean_sentences('english-spanish-both.pkl')\n",
        "train = load_clean_sentences('english-spanish-train.pkl')\n",
        "test = load_clean_sentences('english-spanish-test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jLU2Dvh1vWs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "20444934-7009-484c-ef26-a00acf655008"
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "es_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "es_vocab_size = len(es_tokenizer.word_index) + 1\n",
        "es_length = max_length(dataset[:, 1])\n",
        "print('Spanish Vocabulary Size: %d' % es_vocab_size)\n",
        "print('Spanish Max Length: %d' % (es_length))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 39485\n",
            "English Max Length: 59\n",
            "Spanish Vocabulary Size: 56098\n",
            "Spanish Max Length: 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNosQtxYX8y9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "95c1c232-2a75-4e0f-e445-fecc7f370a0e"
      },
      "source": [
        "import matplotlib.pyplot as plt  \n",
        "a = np.zeros((60,1))\n",
        "for line in train[:,0]:\n",
        "  a[len(line.split())] = a[len(line.split())] + 1\n",
        "plt.plot(a)\n",
        "print(np.sum(a[57:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "107.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uqSUMiJ2hIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_sequences(es_tokenizer, es_length, train[:, 1])\n",
        "# prepare validation data\n",
        "testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_sequences(es_tokenizer, es_length, test[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyg67MQjwXiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbf65238-4e56-4f02-ebe6-ffae1e34f006"
      },
      "source": [
        "train_outY = trainY\n",
        "for index in range(0,len(train_outY)):\n",
        "  k = np.asarray([0]+list(train_outY[index,:-1]))\n",
        "  train_outY[index] = k\n",
        "test_outY = testY\n",
        "for index in range(0,len(test_outY)):\n",
        "  k = np.asarray([0]+list(test_outY[index,:-1]))\n",
        "  test_outY[index] = k\n",
        "trainY = encode_sequences(es_tokenizer, es_length, train[:, 1])\n",
        "testY = encode_sequences(es_tokenizer, es_length, test[:, 1])\n",
        "train_outY.shape,test_outY.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((95000, 59), (5000, 59))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNCbMNzBHTIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units,dropout_rate,eng_embed_dim,es_embed_dim):\n",
        "  x_input1 = Input(shape=(src_timesteps,))\n",
        "  x_input2 = Input(shape=(tar_timesteps,)) \n",
        "\n",
        "  x=Embedding(input_dim=src_vocab,output_dim=eng_embed_dim,embeddings_initializer=\"uniform\",name = \"embedding_en\")(x_input1)\n",
        "  x1=Embedding(input_dim=tar_vocab,output_dim=es_embed_dim,embeddings_initializer=\"uniform\",name = \"embedding_es\")(x_input2)\n",
        "\n",
        "  x_enc,state_h,state_c = LSTM(n_units,return_state = True,recurrent_dropout = dropout_rate)(x)\n",
        "  enc_state = [state_h,state_c]\n",
        "\n",
        "\n",
        "  dec = LSTM(n_units,return_sequences=True,dropout=dropout_rate)(x1,initial_state = enc_state)\n",
        "  out = Dense(tar_vocab,activation='softmax')(dec)\n",
        "  main_model = Model([x_input1,x_input2],out)\n",
        "\n",
        "  return main_model\n",
        "\n",
        "def create_att_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units,dropout_rate,eng_embed_dim,es_embed_dim):\n",
        "  x_input1 = Input(shape=(src_timesteps,))\n",
        "  x_input2 = Input(shape=(tar_timesteps,)) \n",
        "\n",
        "  x=Embedding(input_dim=src_vocab+1,output_dim=eng_embed_dim,embeddings_initializer=\"uniform\",name = \"embedding_en\")(x_input1)\n",
        "  x1=Embedding(input_dim=tar_vocab+1,output_dim=es_embed_dim,embeddings_initializer=\"uniform\",name = \"embedding_es\")(x_input2)\n",
        "\n",
        "  x = Bidirectional(LSTM(int(n_units/2),recurrent_dropout=dropout_rate,return_sequences=True))(x)\n",
        "  x_enc,state_h,state_c = LSTM(n_units,return_state = True,return_sequences=True,recurrent_dropout = dropout_rate)(x)\n",
        "  enc_state = [state_h,state_c]\n",
        "\n",
        "  dec = LSTM(n_units,return_sequences=True,dropout=dropout_rate)(x1,initial_state = enc_state)\n",
        "  dec = LSTM(n_units,return_sequences=True,dropout=dropout_rate)(dec)\n",
        "  # dec = Attention()([dec,x_enc])\n",
        "  # out = Dense(tar_vocab,activation='softmax')(dec)\n",
        "\n",
        "  # luong attention\n",
        "  attention = dot([dec, x_enc], axes=[2, 2])\n",
        "  attention = Activation('softmax', name='attention')(attention)\n",
        "\n",
        "  context = dot([attention, x_enc], axes=[2,1])\n",
        "\n",
        "  decoder_combined_context = concatenate([context, dec])\n",
        "\n",
        "  output = TimeDistributed(Dense(2*n_units, activation=\"tanh\"))(decoder_combined_context)\n",
        "  out = TimeDistributed(Dense(tar_vocab, activation=\"softmax\"))(output)\n",
        "\n",
        "  main_model = Model([x_input1,x_input2],out)\n",
        "\n",
        "  return main_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4fDZ19_6DZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "4e74a3d5-ed78-4c5c-e668-19ebc4ada8ce"
      },
      "source": [
        "n_units = 256\n",
        "eng_embed_dim = 256\n",
        "es_embed_dim = eng_embed_dim\n",
        "dropout_rate = 0.2\n",
        "model = create_att_model(eng_vocab_size, es_vocab_size, eng_length, es_length, n_units,dropout_rate,eng_embed_dim,es_embed_dim)\n",
        "# model = create_att_model(eng_vocab_size, es_vocab_size, eng_length, es_length, n_units,dropout_rate,eng_embed_dim,es_embed_dim)\n",
        "# model = create_bi_model(eng_vocab_size, es_vocab_size, eng_length, es_length, n_units,dropout_rate,eng_embed_dim,es_embed_dim)\n",
        "# model = create_model(eng_vocab_size, es_vocab_size, eng_length, es_length, n_units,dropout_rate,eng_embed_dim,es_embed_dim)\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 59)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_en (Embedding)        (None, 59, 256)      10108416    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 59)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 59, 256)      394240      embedding_en[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_es (Embedding)        (None, 59, 256)      14361344    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 59, 256), (N 525312      bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 59, 256)      525312      embedding_es[0][0]               \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 59, 256)      525312      lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 59, 59)       0           lstm_3[0][0]                     \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention (Activation)          (None, 59, 59)       0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 59, 256)      0           attention[0][0]                  \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 59, 512)      0           dot_1[0][0]                      \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 59, 512)      262656      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 59, 56098)    28778274    time_distributed[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 55,480,866\n",
            "Trainable params: 55,480,866\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNLqaOdL4xKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "187befb2-9f47-43c2-ab94-f4ec7573db6d"
      },
      "source": [
        "%cd /content/\n",
        "version = \"0\"\n",
        "filename = 'model'+str(n_units)+'_'+str(version)+'.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "los = SparseCategoricalCrossentropy()\n",
        "# model = load_model('model100_0.h5')\n",
        "opt = optimizers.Adam(lr=0.001)\n",
        "model.compile(loss=los,optimizer=opt,metrics=['accuracy'])\n",
        "model.fit([trainX,train_outY], trainY, epochs=30, batch_size=256, validation_data=([testX,test_outY], testY), callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Epoch 1/30\n",
            "372/372 [==============================] - ETA: 0s - loss: 0.8073 - accuracy: 0.8574\n",
            "Epoch 00001: val_loss improved from inf to 0.97146, saving model to model256_0.h5\n",
            "372/372 [==============================] - 876s 2s/step - loss: 0.8073 - accuracy: 0.8574 - val_loss: 0.9715 - val_accuracy: 0.8502\n",
            "Epoch 2/30\n",
            "372/372 [==============================] - ETA: 0s - loss: 0.7195 - accuracy: 0.8684\n",
            "Epoch 00002: val_loss improved from 0.97146 to 0.94072, saving model to model256_0.h5\n",
            "372/372 [==============================] - 873s 2s/step - loss: 0.7195 - accuracy: 0.8684 - val_loss: 0.9407 - val_accuracy: 0.8553\n",
            "Epoch 3/30\n",
            "372/372 [==============================] - ETA: 0s - loss: 0.6512 - accuracy: 0.8780\n",
            "Epoch 00003: val_loss improved from 0.94072 to 0.92377, saving model to model256_0.h5\n",
            "372/372 [==============================] - 874s 2s/step - loss: 0.6512 - accuracy: 0.8780 - val_loss: 0.9238 - val_accuracy: 0.8580\n",
            "Epoch 4/30\n",
            " 12/372 [..............................] - ETA: 12:46 - loss: 0.5874 - accuracy: 0.8869"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-532790d78b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_outY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_outY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1sXLdd_8vnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "a0d16909-750b-4ccc-8784-226db28a459f"
      },
      "source": [
        "lim = 100000\n",
        "opt = optimizers.Adam(lr=0.0001)\n",
        "# model = load_model('model150_0.h5')\n",
        "model.compile(loss=los,optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "model.fit([trainX[:lim],train_outY[:lim]], trainY[:lim], epochs=7, batch_size=256, validation_data=([testX,test_outY], testY), callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "372/372 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.8918\n",
            "Epoch 00001: val_loss improved from 0.92377 to 0.89894, saving model to model256_0.h5\n",
            "372/372 [==============================] - 881s 2s/step - loss: 0.5628 - accuracy: 0.8918 - val_loss: 0.8989 - val_accuracy: 0.8617\n",
            "Epoch 2/7\n",
            "372/372 [==============================] - ETA: 0s - loss: 0.5481 - accuracy: 0.8944\n",
            "Epoch 00002: val_loss improved from 0.89894 to 0.89599, saving model to model256_0.h5\n",
            "372/372 [==============================] - 877s 2s/step - loss: 0.5481 - accuracy: 0.8944 - val_loss: 0.8960 - val_accuracy: 0.8621\n",
            "Epoch 3/7\n",
            "166/372 [============>.................] - ETA: 7:55 - loss: 0.5394 - accuracy: 0.8958"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-01224178b6e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_outY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_outY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGrv561lKZ3n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5095ef8-5bb2-4087-d1f9-5435e84026bf"
      },
      "source": [
        "%cd /content/\n",
        "model = load_model(\"model256_0.h5\")\n",
        "performance(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "1\n",
            "source: but whats happened is our technology has removed the checks and balances on our population growth                                           \n",
            "actual: pero lo que ha ocurrido es que nuestra tecnologa ha quitado los controles y equilibrios en el crecimiento popular - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "predicted: pero lo que ha pasado es nuestra tecnologa se ha quitado la carga y la lucha en nuestro crecimiento de la poblacin - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "source: you know hes thinking what am i getting myself into                                                 \n",
            "actual: ustedes sabenl estaba pensando en qu me estoy involuncrando - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "predicted: saben l es pensar qu estoy haciendo - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "3\n",
            "source: and that was something that i went through you know and im still going through                                            \n",
            "actual: y que fue algo por lo que pas saben y que an lo estoy pasando - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "predicted: y eso fue algo que fui a travs de ustedes saben y yo sigo adelante - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "4\n",
            "source: the president needs to stand up and say at the end of a healthcare reform debate our goal as a country is to move percent of care out of institutions clinics hospitals and nursing homes to the home in years                   \n",
            "actual: el presidente debe alzarse y decir al final del debate sobre la reforma de salud nuestro objetivo como pas es trasladar el de la asistencia de las instituciones clnicas hospitales y residencias a los hogares en aos - - - - - - - - - - - - - - - - - - - - - -\n",
            "predicted: el presidente necesita ser acudir y decir en el final de una reforma de salud de la salud el debate como el objetivo es mover el por ciento de la creacin de las clulas de las clulas de las clulas de las clulas de la casa - - - - - - - - - - - - -\n",
            "5\n",
            "source: you can say the small cookie has godiva chocolate bits in it but it doesnt work they want the big cookie                                      \n",
            "actual: puedes decirles que la galleta pequea tiene trozos de chocolate godiva pero eso no funciona ellos quieren la galleta grande - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "predicted: pueden decir que la pequea galleta de la mujer ha sido enfriado el chocolate pero no funciona quieren que las grandes compaas - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "6\n",
            "source: im not going to talk about the skeptical environmentalist probably thats also a good choice                                            \n",
            "actual: no les voy a hablar del ambientalista escptico probablemente tambin un buen tema - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "predicted: no voy a hablar de la bestialidad macro de los romanos probablemente es una buena eleccin - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "7\n",
            "source: its actually where it all started for me                                                   \n",
            "actual: de hecho es donde todo empez para m - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "predicted: es por eso todos empezamos por m - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "8\n",
            "source: that little guy is frisky now youve seen video like this on tv a lot and its very intimidating and i think it gives the wrong impression about sharks                              \n",
            "actual: ese chico est un poco juguetn ahora han visto muchos videos como este en la tv y es intimidante y creo que da una impresin equivocada acerca de los tiburones - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "predicted: ese pequeo tipo est de pie ahora mismo que has visto video como esta de la tv mucho y es muy intimidante y creo que le da la impresin equivocada sobre la calle - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "9\n",
            "source: so oh theres another shot of my beard                                                   \n",
            "actual: entonces oh ah hay otra foto de mi barba - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "predicted: oh hay otra toma de furby - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "125\n",
            "0.10102270027384012 0.31689516438034254 0.46801170802169006 0.562419435201387\n",
            "0.01680672268907563 0.12964074471043288 0.25966259996470964 0.360056585428503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKp4xWcCJ--d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "67ef74ac-f43b-4472-8b63-895e77d2a269"
      },
      "source": [
        "lim = 100000\n",
        "opt = optimizers.Adam(lr=0.0001)\n",
        "model = load_model('model150_0.h5')\n",
        "model.compile(loss=los,optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "model.fit([trainX[:lim],train_outY[:lim]], trainY[:lim], epochs=10, batch_size=256, validation_data=([testX,test_outY], testY), callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/10\n",
            "372/372 [==============================] - ETA: 0s - loss: 0.5596 - accuracy: 0.8929\n",
            "Epoch 00001: val_loss improved from 0.90288 to 0.89247, saving model to model150_0.h5\n",
            "372/372 [==============================] - 766s 2s/step - loss: 0.5596 - accuracy: 0.8929 - val_loss: 0.8925 - val_accuracy: 0.8631\n",
            "Epoch 2/10\n",
            "372/372 [==============================] - ETA: 0s - loss: 0.5506 - accuracy: 0.8946\n",
            "Epoch 00002: val_loss improved from 0.89247 to 0.88979, saving model to model150_0.h5\n",
            "372/372 [==============================] - 769s 2s/step - loss: 0.5506 - accuracy: 0.8946 - val_loss: 0.8898 - val_accuracy: 0.8635\n",
            "Epoch 3/10\n",
            "372/372 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.8954\n",
            "Epoch 00003: val_loss did not improve from 0.88979\n",
            "372/372 [==============================] - 766s 2s/step - loss: 0.5455 - accuracy: 0.8954 - val_loss: 0.8900 - val_accuracy: 0.8635\n",
            "Epoch 4/10\n",
            "301/372 [=======================>......] - ETA: 2:23 - loss: 0.5412 - accuracy: 0.8962"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dcTibped_PJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/\n",
        "model = load_model(\"model150_0.h5\")\n",
        "performance(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y_P1XCuN9kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_string = \"hello i will dance\"\n",
        "t = np.asarray(test_string,dtype='<U275').reshape((1,))\n",
        "tX = encode_sequences(eng_tokenizer, eng_length, t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4v_D1b0LmDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "235f7f52-0205-412f-e747-2fc4bcfdbe20"
      },
      "source": [
        "x_input11=tX.reshape(1,eng_length)\n",
        "x_input21=np.zeros((1,es_length)) \n",
        "predicted = []\n",
        "for i in range(0,es_length):\n",
        "  output = model.predict([x_input11,x_input21])\n",
        "  out1=np.argmax(output,axis=2)\n",
        "  a = out1[0,i]\n",
        "  if i !=es_length-1:\n",
        "    x_input21[0,i+1]=a\n",
        "  if a==0:\n",
        "    break\n",
        "prediction = x_input21[0].tolist()\n",
        "prediction.append(a)\n",
        "for w in prediction[1:]:\n",
        "  try:\n",
        "    predicted.append(es_tokenizer.index_word[w])\n",
        "  except:\n",
        "    predicted.append('')\n",
        "prediction = \" \".join(predicted)\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hola yo dedo                                                        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k8eWxUvtu8V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ffe46b7-31fd-47ef-e9bf-35cef7fa6b93"
      },
      "source": [
        "for t in out1[0]:\n",
        "  try:\n",
        "    print(es_tokenizer.index_word[t],end=\" \")\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hola yo dedo "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vv__har6AEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3bd8786-8615-49ac-b1b6-ecaf510253fb"
      },
      "source": [
        "####BEST ONE BY FAR###############\n",
        "model = load_model(\"model100_0.h5\")\n",
        "print(\"model name  : model100_0.h5\")\n",
        "print(model.summary())\n",
        "performance(model)\n",
        "model = load_model(\"model100_1.h5\")\n",
        "print(\"model name  : model100_1.h5\")\n",
        "print(model.summary())\n",
        "performance(model)\n",
        "model = load_model(\"model100_2.h5\")\n",
        "print(\"model name  : model100_2.h5\")\n",
        "print(model.summary())\n",
        "performance(model)\n",
        "model = load_model(\"model50_2.h5\")\n",
        "print(\"model name  : model50_2.h5\")\n",
        "print(model.summary())\n",
        "performance(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "model name  : model200_0.h5\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_15 (InputLayer)           [(None, 9)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_16 (InputLayer)           [(None, 14)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_en (Embedding)        (None, 9, 100)       851700      input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_es (Embedding)        (None, 14, 100)      1604300     input_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_14 (LSTM)                  [(None, 200), (None, 240800      embedding_en[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_15 (LSTM)                  (None, 14, 200)      240800      embedding_es[0][0]               \n",
            "                                                                 lstm_14[0][1]                    \n",
            "                                                                 lstm_14[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 14, 16043)    3224643     lstm_15[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 6,162,243\n",
            "Trainable params: 6,162,243\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "source: i guessed right      \n",
            "actual: acerte - - - - - - - - - - - - -\n",
            "predicted: lo adivine correctamente - - - - - - - - - - -\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "source: i think hes a great writer   \n",
            "actual: pienso que es un gran escritor - - - - - - - -\n",
            "predicted: creo que es una gran escritor - - - - - - - -\n",
            "source: choose the one you like    \n",
            "actual: elige el que te guste - - - - - - - - -\n",
            "predicted: elige usted el que quiere - - - - - - - - -\n",
            "source: he knows how to tell lies   \n",
            "actual: el sabe decir mentiras - - - - - - - - - -\n",
            "predicted: el sabe como decir - - - - - - - - - -\n",
            "source: he flew a kite     \n",
            "actual: el elevo un volantin - - - - - - - - - -\n",
            "predicted: el atrapo un volantin - - - - - - - - - -\n",
            "source: i want tom to win    \n",
            "actual: quiero que tom gane - - - - - - - - - -\n",
            "predicted: quiero que tom ganara - - - - - - - - - -\n",
            "source: ill probably use it again    \n",
            "actual: probablemente lo voy a usar de nuevo - - - - - - -\n",
            "predicted: yo solia haber leido hace - - - - - - - - -\n",
            "source: you need to be there    \n",
            "actual: necesitas estar ahi - - - - - - - - - - -\n",
            "predicted: necesitas estar ahi - - - - - - - - - - -\n",
            "source: i found you      \n",
            "actual: ya te encontre - - - - - - - - - - -\n",
            "predicted: te encontre - - - - - - - - - - - -\n",
            "70\n",
            "0.2589602478147615 0.505556996077853 0.6365111252864258 0.7097714985746765\n",
            "0.09374999999999999 0.30618621784789724 0.4578787844531212 0.5533409598501607\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "model name  : model100_1.h5\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 9)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 14)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_en (Embedding)        (None, 9, 100)       851700      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_es (Embedding)        (None, 14, 100)      1604300     input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   [(None, 100), (None, 80400       embedding_en[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 14, 100)      80400       embedding_es[0][0]               \n",
            "                                                                 lstm_6[0][1]                     \n",
            "                                                                 lstm_6[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 14, 16043)    1620343     lstm_7[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 4,237,143\n",
            "Trainable params: 1,781,143\n",
            "Non-trainable params: 2,456,000\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "source: i guessed right      \n",
            "actual: acerte - - - - - - - - - - - - -\n",
            "predicted: lo adivine correctamente - - - - - - - - - - -\n",
            "source: i think hes a great writer   \n",
            "actual: pienso que es un gran escritor - - - - - - - -\n",
            "predicted: yo pienso que ella es un gran escritor - - - - - -\n",
            "source: choose the one you like    \n",
            "actual: elige el que te guste - - - - - - - - -\n",
            "predicted: elige a ella con ella - - - - - - - - -\n",
            "source: he knows how to tell lies   \n",
            "actual: el sabe decir mentiras - - - - - - - - - -\n",
            "predicted: el sabe como pudo nadar - - - - - - - - -\n",
            "source: he flew a kite     \n",
            "actual: el elevo un volantin - - - - - - - - - -\n",
            "predicted: el elevo un minuto - - - - - - - - - -\n",
            "source: i want tom to win    \n",
            "actual: quiero que tom gane - - - - - - - - - -\n",
            "predicted: quiero que tom gane - - - - - - - - - -\n",
            "source: ill probably use it again    \n",
            "actual: probablemente lo voy a usar de nuevo - - - - - - -\n",
            "predicted: lo volvere solo - - - - - - - - - - -\n",
            "source: you need to be there    \n",
            "actual: necesitas estar ahi - - - - - - - - - - -\n",
            "predicted: tienes que estar ahi - - - - - - - - - -\n",
            "source: i found you      \n",
            "actual: ya te encontre - - - - - - - - - - -\n",
            "predicted: te encontre - - - - - - - - - - - -\n",
            "70\n",
            "0.263139231990148 0.5103180338512322 0.6406961201164779 0.7134039827711848\n",
            "0.15151515151515155 0.3892494720807615 0.5364762511356723 0.6238986072117501\n",
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "model name  : model100_2.h5\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, 9)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           [(None, 14)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_en (Embedding)        (None, 9, 150)       1277550     input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_es (Embedding)        (None, 14, 150)      2406450     input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  [(None, 100), (None, 100400      embedding_en[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  (None, 14, 100)      100400      embedding_es[0][0]               \n",
            "                                                                 lstm_10[0][1]                    \n",
            "                                                                 lstm_10[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 14, 16043)    1620343     lstm_11[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,505,143\n",
            "Trainable params: 5,505,143\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "source: i guessed right      \n",
            "actual: acerte - - - - - - - - - - - - -\n",
            "predicted: lo adivine correctamente - - - - - - - - - - -\n",
            "source: i think hes a great writer   \n",
            "actual: pienso que es un gran escritor - - - - - - - -\n",
            "predicted: creo que es un buen abogado - - - - - - - -\n",
            "source: choose the one you like    \n",
            "actual: elige el que te guste - - - - - - - - -\n",
            "predicted: elige cuando quieras - - - - - - - - - - -\n",
            "source: he knows how to tell lies   \n",
            "actual: el sabe decir mentiras - - - - - - - - - -\n",
            "predicted: el sabe como si - - - - - - - - - -\n",
            "source: he flew a kite     \n",
            "actual: el elevo un volantin - - - - - - - - - -\n",
            "predicted: el construyo un camion - - - - - - - - - -\n",
            "source: i want tom to win    \n",
            "actual: quiero que tom gane - - - - - - - - - -\n",
            "predicted: quiero que tom ganara - - - - - - - - - -\n",
            "source: ill probably use it again    \n",
            "actual: probablemente lo voy a usar de nuevo - - - - - - -\n",
            "predicted: probablemente necesite manana - - - - - - - - - - -\n",
            "source: you need to be there    \n",
            "actual: necesitas estar ahi - - - - - - - - - - -\n",
            "predicted: tienes que estar alli - - - - - - - - - -\n",
            "source: i found you      \n",
            "actual: ya te encontre - - - - - - - - - - -\n",
            "predicted: te encontre - - - - - - - - - - - -\n",
            "70\n",
            "0.2614367273361869 0.5085746697443524 0.6392210354120667 0.7121468148912449\n",
            "0.1388888888888889 0.37267799624996495 0.521291059062705 0.6104735835807844\n",
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "model name  : model50_2.h5\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 9)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           [(None, 14)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_en (Embedding)        (None, 9, 100)       851700      input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_es (Embedding)        (None, 14, 100)      1604300     input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_12 (LSTM)                  [(None, 50), (None,  30200       embedding_en[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_13 (LSTM)                  (None, 14, 50)       30200       embedding_es[0][0]               \n",
            "                                                                 lstm_12[0][1]                    \n",
            "                                                                 lstm_12[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 14, 16043)    818193      lstm_13[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,334,593\n",
            "Trainable params: 878,593\n",
            "Non-trainable params: 2,456,000\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "source: i guessed right      \n",
            "actual: acerte - - - - - - - - - - - - -\n",
            "predicted: me agarre un vistazo - - - - - - - - - -\n",
            "source: i think hes a great writer   \n",
            "actual: pienso que es un gran escritor - - - - - - - -\n",
            "predicted: creo que es una buena nadadora - - - - - - - -\n",
            "source: choose the one you like    \n",
            "actual: elige el que te guste - - - - - - - - -\n",
            "predicted: di a tom te llamo - - - - - - - - -\n",
            "source: he knows how to tell lies   \n",
            "actual: el sabe decir mentiras - - - - - - - - - -\n",
            "predicted: el sabe lo que le paso - - - - - - - -\n",
            "source: he flew a kite     \n",
            "actual: el elevo un volantin - - - - - - - - - -\n",
            "predicted: el jurado ha sido una maldicion - - - - - - - -\n",
            "source: i want tom to win    \n",
            "actual: quiero que tom gane - - - - - - - - - -\n",
            "predicted: quiero que tom - - - - - - - - - - -\n",
            "source: ill probably use it again    \n",
            "actual: probablemente lo voy a usar de nuevo - - - - - - -\n",
            "predicted: lo hare un trabajo por eso - - - - - - - -\n",
            "source: you need to be there    \n",
            "actual: necesitas estar ahi - - - - - - - - - - -\n",
            "predicted: necesitas estar alli - - - - - - - - - - -\n",
            "source: i found you      \n",
            "actual: ya te encontre - - - - - - - - - - -\n",
            "predicted: lo hare - - - - - - - - - - - -\n",
            "70\n",
            "0.24950677507770364 0.49687666342347875 0.6295011158915041 0.7039386122617124\n",
            "0.14285714285714285 0.3779644730092272 0.5261597794341889 0.6147881529512643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIpj32XpTjgM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9ea73acd-479a-4e65-a752-289ac015193c"
      },
      "source": [
        "from shutil import copyfile\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!mkdir cs224n_nmt\n",
        "copyfile(\"/content/model256_0.h5\",\"/content/drive/My Drive/cs224n_nmt/model256_0.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "mkdir: cannot create directory ‘cs224n_nmt’: File exists\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/cs224n_nmt/model256_0.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8zCM2HciU4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}