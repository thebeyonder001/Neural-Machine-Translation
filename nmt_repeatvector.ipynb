{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt_repeatvector.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITNbe4E7yuaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "from pickle import load\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_9FszqrulEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical,plot_model\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W64rEcEgUlZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PCZGpDRuvGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "18a7dbf1-13e7-42b4-a8a7-a11dd17fd667"
      },
      "source": [
        "!wget http://www.manythings.org/anki/spa-eng.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-16 06:26:09--  http://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 172.67.173.198, 104.24.108.196, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4781548 (4.6M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   4.56M  1.61MB/s    in 2.8s    \n",
            "\n",
            "2020-06-16 06:26:12 (1.61 MB/s) - ‘spa-eng.zip’ saved [4781548/4781548]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7AK8k_dvCMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3347e75f-645d-4194-9d65-a0bf849b0b92"
      },
      "source": [
        "!unzip spa-eng.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaZkjW7axxZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t')[:2] for line in  lines]\n",
        "\treturn pairs\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay3LEQDLysvk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "691159d1-eaa0-4a8f-edf3-11913cb7bcdf"
      },
      "source": [
        "# load dataset\n",
        "filename = 'spa.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-spanish.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-spanish.pkl\n",
            "[go] => [ve]\n",
            "[go] => [vete]\n",
            "[go] => [vaya]\n",
            "[go] => [vayase]\n",
            "[hi] => [hola]\n",
            "[run] => [corre]\n",
            "[run] => [corran]\n",
            "[run] => [corra]\n",
            "[run] => [corred]\n",
            "[run] => [corred]\n",
            "[who] => [quien]\n",
            "[wow] => [orale]\n",
            "[fire] => [fuego]\n",
            "[fire] => [incendio]\n",
            "[fire] => [disparad]\n",
            "[help] => [ayuda]\n",
            "[help] => [socorro auxilio]\n",
            "[help] => [auxilio]\n",
            "[jump] => [salta]\n",
            "[jump] => [salte]\n",
            "[stop] => [parad]\n",
            "[stop] => [para]\n",
            "[stop] => [pare]\n",
            "[wait] => [espera]\n",
            "[wait] => [esperen]\n",
            "[go on] => [continua]\n",
            "[go on] => [continue]\n",
            "[hello] => [hola]\n",
            "[i hid] => [me oculte]\n",
            "[i hid] => [me escondi]\n",
            "[i hid] => [me ocultaba]\n",
            "[i hid] => [me escondia]\n",
            "[i ran] => [corri]\n",
            "[i ran] => [corria]\n",
            "[i try] => [lo intento]\n",
            "[i won] => [he ganado]\n",
            "[oh no] => [oh no]\n",
            "[relax] => [tomatelo con soda]\n",
            "[shoot] => [fuego]\n",
            "[shoot] => [disparad]\n",
            "[shoot] => [disparen]\n",
            "[shoot] => [dispara]\n",
            "[shoot] => [dispara]\n",
            "[shoot] => [dispare]\n",
            "[smile] => [sonrie]\n",
            "[attack] => [al ataque]\n",
            "[attack] => [atacad]\n",
            "[attack] => [ataque]\n",
            "[attack] => [ataquen]\n",
            "[attack] => [ataca]\n",
            "[get up] => [levanta]\n",
            "[go now] => [ve ahora mismo]\n",
            "[go now] => [id ahora mismo]\n",
            "[go now] => [vaya ahora mismo]\n",
            "[go now] => [vayan ahora mismo]\n",
            "[go now] => [ve ya]\n",
            "[go now] => [id ya]\n",
            "[go now] => [vaya ya]\n",
            "[go now] => [vayan ya]\n",
            "[got it] => [lo tengo]\n",
            "[got it] => [lo pillas]\n",
            "[got it] => [entendiste]\n",
            "[he ran] => [el corrio]\n",
            "[hop in] => [metete adentro]\n",
            "[hug me] => [abrazame]\n",
            "[i care] => [me preocupo]\n",
            "[i fell] => [me cai]\n",
            "[i fled] => [hui]\n",
            "[i fled] => [me escape]\n",
            "[i fled] => [huia]\n",
            "[i fled] => [me escapaba]\n",
            "[i know] => [yo lo se]\n",
            "[i left] => [sali]\n",
            "[i lied] => [menti]\n",
            "[i lost] => [perdi]\n",
            "[i quit] => [dimito]\n",
            "[i quit] => [renuncie]\n",
            "[i quit] => [lo dejo]\n",
            "[i sang] => [cante]\n",
            "[i wept] => [llore]\n",
            "[i wept] => [lloraba]\n",
            "[i work] => [estoy trabajando]\n",
            "[im] => [tengo diecinueve]\n",
            "[im up] => [estoy levantado]\n",
            "[listen] => [escucha]\n",
            "[listen] => [escuche]\n",
            "[listen] => [escuchen]\n",
            "[no way] => [no puede ser]\n",
            "[no way] => [de ninguna manera]\n",
            "[no way] => [de ninguna manera]\n",
            "[no way] => [imposible]\n",
            "[no way] => [de ningun modo]\n",
            "[no way] => [de eso nada]\n",
            "[no way] => [ni cagando]\n",
            "[no way] => [mangos]\n",
            "[no way] => [minga]\n",
            "[no way] => [ni en pedo]\n",
            "[really] => [en serio]\n",
            "[really] => [la verdad]\n",
            "[thanks] => [gracias]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq3t33iXy4Xp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "dcdb5524-220d-4e42-cf3f-6f56866277e8"
      },
      "source": [
        "for i in range(10):\n",
        "\tprint(clean_pairs[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['go' 've']\n",
            "['go' 'vete']\n",
            "['go' 'vaya']\n",
            "['go' 'vayase']\n",
            "['hi' 'hola']\n",
            "['run' 'corre']\n",
            "['run' 'corran']\n",
            "['run' 'corra']\n",
            "['run' 'corred']\n",
            "['run' 'corred']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXimLkuLz7vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "#EVALUATION FUNCTIONS\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        " \n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "  predict = model.predict(source, verbose=0)[0]\n",
        "  integers = []\n",
        "  for pred in predict:\n",
        "    integers.append(np.argmax(pred))\n",
        "  target = []\n",
        "  for i in integers:\n",
        "    word = word_for_id(i, tokenizer)\n",
        "    if word is None:\n",
        "      break\n",
        "    target.append(word)\n",
        "  return ' '.join(target)\n",
        " \n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "  actual = []\n",
        "  predicted = []\n",
        "  b1=0\n",
        "  b2=0\n",
        "  b3=0\n",
        "  b4=0\n",
        "  count = 0\n",
        "  for i, source in enumerate(sources):    # translate encoded source text\n",
        "    source = source.reshape((1, source.shape[0]))\n",
        "    translated = predict_sequence(model, tokenizer, source)\n",
        "    raw_src, raw_tgt = raw_dataset[i]\n",
        "    if i<10:\n",
        "      print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_tgt, translated))\n",
        "      count = count+1\n",
        "    else:\n",
        "      break\n",
        "    actual.append([raw_tgt.split()])\n",
        "    predicted.append(translated.split())\n",
        "    b1=b1+ corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
        "    b2=b2+  corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
        "    b3=b3+ corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0))\n",
        "    b4=b4+ corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  print(b1/count,b2/count,b3/count,b4/count)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j898T8iHfEvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "8c0d2f12-b5e8-40cd-c9a2-c9c8770956e2"
      },
      "source": [
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-spanish.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 100000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:49000], dataset[49000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-spanish-both.pkl')\n",
        "save_clean_data(train, 'english-spanish-train.pkl')\n",
        "save_clean_data(test, 'english-spanish-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-spanish-both.pkl\n",
            "Saved: english-spanish-train.pkl\n",
            "Saved: english-spanish-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoGOh1dA1PTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = load_clean_sentences('english-spanish-both.pkl')\n",
        "train = load_clean_sentences('english-spanish-train.pkl')\n",
        "test = load_clean_sentences('english-spanish-test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jLU2Dvh1vWs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "1fa96867-dcf2-48a9-f482-a8242c3039f1"
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "es_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "es_vocab_size = len(es_tokenizer.word_index) + 1\n",
        "es_length = max_length(dataset[:, 1])\n",
        "print('Spanish Vocabulary Size: %d' % es_vocab_size)\n",
        "print('Spanish Max Length: %d' % (es_length))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 10821\n",
            "English Max Length: 11\n",
            "Spanish Vocabulary Size: 20432\n",
            "Spanish Max Length: 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uqSUMiJ2hIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_sequences(es_tokenizer, es_length, train[:, 1])\n",
        "#trainY = encode_output(trainY, es_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_sequences(es_tokenizer, es_length, test[:, 1])\n",
        "#testY = encode_output(testY, es_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4xa3m8JtPHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "74667b2c-0c45-458b-e749-2f30a867dce9"
      },
      "source": [
        "!pip install keras-self-attention\n",
        "from keras_self_attention import SeqSelfAttention"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.6/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.18.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZOk6uEK28Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "def define_model_drop(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units,dropout_rate,embed_dim):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, embed_dim, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units,dropout = dropout_rate))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True,dropout = dropout_rate))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "def define_model_bi_drop(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units,dropout_rate,embed_dim):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, embed_dim, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(Bidirectional(LSTM(n_units,dropout = dropout_rate)))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True,dropout = dropout_rate))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "def define_model_bi_1_drop(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units,n_units1,dropout_rate,embed_dim):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(src_vocab, embed_dim, input_length=src_timesteps, mask_zero=True))\n",
        "  model.add(Bidirectional(LSTM(n_units,dropout = dropout_rate,return_sequences=True)))\n",
        "  model.add(LSTM(n_units1,dropout=dropout_rate))\n",
        "  model.add(RepeatVector(tar_timesteps))\n",
        "  model.add(LSTM(n_units, return_sequences=True,dropout = dropout_rate))\n",
        "  model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "  return model\n",
        "def define_model_bi_att_drop(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units,dropout_rate,embed_dim):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(src_vocab, embed_dim, input_length=src_timesteps, mask_zero=True))\n",
        "  model.add(Bidirectional(LSTM(n_units,dropout = dropout_rate,return_sequences = True)))\n",
        "  model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "  model.add(RepeatVector(tar_timesteps))\n",
        "  model.add(LSTM(n_units, return_sequences=True,dropout = dropout_rate))\n",
        "  model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4fDZ19_6DZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "bae3726a-cc98-4853-e9d0-0936fa989006"
      },
      "source": [
        "n_units = 200\n",
        "n_units1= 100\n",
        "dropout_rate = 0.2\n",
        "embed_dim = 300\n",
        "model = define_model_bi_1_drop(eng_vocab_size, es_vocab_size, eng_length, es_length, n_units,n_units1,dropout_rate,embed_dim)\n",
        "# model = define_model_att_drop(eng_vocab_size, es_vocab_size, eng_length, es_length, n_units,dropout_rate,embed_dim)\n",
        "# model = define_model_bi_drop(eng_vocab_size, es_vocab_size, eng_length, es_length, n_units,dropout_rate,embed_dim)\n",
        "# model = define_model_drop(eng_vocab_size, es_vocab_size, eng_length, es_length, n_units,dropout_rate,embed_dim)\n",
        "# model = define_model(eng_vocab_size, es_vocab_size, eng_length, es_length, n_units)\n",
        "print(model.summary())\n",
        "#plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 11, 300)           3246300   \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 11, 400)           801600    \n",
            "_________________________________________________________________\n",
            "lstm_24 (LSTM)               (None, 100)               200400    \n",
            "_________________________________________________________________\n",
            "repeat_vector_8 (RepeatVecto (None, 15, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_25 (LSTM)               (None, 15, 200)           240800    \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 15, 20432)         4106832   \n",
            "=================================================================\n",
            "Total params: 8,595,932\n",
            "Trainable params: 8,595,932\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piieP8tUrQNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def leave_embed(model):\n",
        "  for l in model.layers:\n",
        "    if \"embedding\" in l.name:\n",
        "      l.trainable = False\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNLqaOdL4xKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "7153c601-be66-43b1-84f0-49ffdeaff90f"
      },
      "source": [
        "%cd /content/\n",
        "version = \"bi_100_drop_0\"\n",
        "filename = 'model'+str(n_units)+'_'+str(version)+'.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "los = SparseCategoricalCrossentropy()\n",
        "opt = optimizers.Adam(lr=0.001)\n",
        "opt1 = optimizers.RMSprop(0.5)\n",
        "model = load_model('model200_bi_100_drop_0.h5')\n",
        "model = leave_embed(model)\n",
        "model.compile(loss=los,optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Epoch 1/30\n",
            "765/766 [============================>.] - ETA: 0s - loss: 1.2431 - accuracy: 0.7450\n",
            "Epoch 00001: val_loss improved from inf to 1.69535, saving model to model200_bi_100_drop_0.h5\n",
            "766/766 [==============================] - 76s 99ms/step - loss: 1.2430 - accuracy: 0.7450 - val_loss: 1.6954 - val_accuracy: 0.7389\n",
            "Epoch 2/30\n",
            "765/766 [============================>.] - ETA: 0s - loss: 1.2224 - accuracy: 0.7466\n",
            "Epoch 00002: val_loss did not improve from 1.69535\n",
            "766/766 [==============================] - 72s 94ms/step - loss: 1.2224 - accuracy: 0.7466 - val_loss: 1.6976 - val_accuracy: 0.7392\n",
            "Epoch 3/30\n",
            "765/766 [============================>.] - ETA: 0s - loss: 1.2075 - accuracy: 0.7479\n",
            "Epoch 00003: val_loss did not improve from 1.69535\n",
            "766/766 [==============================] - 72s 94ms/step - loss: 1.2075 - accuracy: 0.7479 - val_loss: 1.6983 - val_accuracy: 0.7395\n",
            "Epoch 4/30\n",
            "157/766 [=====>........................] - ETA: 36s - loss: 1.1855 - accuracy: 0.7507"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-ddef940510a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkfFHSo_qeE_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "2069e643-22c3-400d-ea41-e2c60d5cd9be"
      },
      "source": [
        "model = load_model('model150_bi_150_75_drop_0.h5')\n",
        "\n",
        "print(\"TRAIN : \\n\")\n",
        "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "evaluate_model(model, es_tokenizer, trainX, train)\n",
        "print(\"\\nTEST : \\n\")\n",
        "testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "evaluate_model(model, es_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN : \n",
            "\n",
            "src=[would that have made it better], target=[eso se hubiera hecho mejor], predicted=[eso mejor haber hecho]\n",
            "src=[she got up to answer the phone], target=[ella se levanto para contestar al telefono], predicted=[ella le a a telefono telefono]\n",
            "src=[dont leave the light on], target=[no dejes la luz encendida], predicted=[no dejes abiertas luz encendida]\n",
            "src=[it is no use crying over spilt milk], target=[no tiene caso llorar sobre leche derramada], predicted=[no no no de en en derramada derramada]\n",
            "src=[tom is really good at math], target=[tom es realmente bueno en matematicas], predicted=[tom es bueno bueno de matematicas matematicas]\n",
            "src=[tom has his own room], target=[tom tiene su propio dormitorio], predicted=[tom tiene ordenando cuarto dormitorio]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[why didnt you stay with tom], target=[por que no te quedaste con tom], predicted=[por que no a a a tom]\n",
            "src=[tom insisted on helping mary], target=[tom insistio en ayudar a mary], predicted=[tom insistio pedirle con mary]\n",
            "src=[have you ever ridden a bicycle], target=[alguna vez te has subido a una bicicleta], predicted=[has vez has vez vez vez habitacion]\n",
            "src=[tom seems to be as fit as a fiddle], target=[tom parece estar tan fino como un violin afinado], predicted=[tom parece que que fino fierro escritor]\n",
            "0.4977703502125119 0.3387268006641949 0.3479552987533751 0.42412950757348283\n",
            "\n",
            "TEST : \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[could i have a piece of cheesecake], target=[me podrian dar un trozo de tarta de queso], predicted=[podrian podrian un pedazo pedazo de de]\n",
            "src=[he finally decided to get married], target=[finalmente decidio casarse], predicted=[el se a a]\n",
            "src=[that was magic], target=[eso fue magia], predicted=[eso es magico]\n",
            "src=[is that too much to ask for], target=[es mucho pedir], predicted=[eso es demasiado pena para]\n",
            "src=[go straight along this street], target=[siga derecho por esta calle], predicted=[vete recto en la]\n",
            "src=[i wonder if tom will be at marys party], target=[me pregunto si tom ira a la fiesta de mary], predicted=[me pregunto a tom tom que a a la]\n",
            "src=[you have three pens], target=[tienes tres lapices], predicted=[tienes tres]\n",
            "src=[they are in the teachers room], target=[ellos estan en la sala de profesores], predicted=[ellos estan en el pieza]\n",
            "src=[i dont intend do that], target=[no trato de hacerlo], predicted=[no no hacer eso]\n",
            "src=[i wonder if anything happened to him], target=[me pregunto si le paso algo], predicted=[me pregunto si no no]\n",
            "0.3470261534415816 0.38012575396546167 0.4302205843680496 0.5013405410272603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo8QvEQor982",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<table width=\"900\" align = \"left\" height=\"200\" borderwidth = \"3\">\n",
        "  <tr>\n",
        "    <th>DATA_SIZE</th><th>n_units</th><th>dropout</th>\n",
        "    <th>Bidirectional</th><th>embed_dim</th><th>performance</th>\n",
        "    <th>bleu score</th>\n",
        "  </tr>\n",
        "  <tr><td>10000 (TOY)</td><td>256</td><td>NA</td>\n",
        "    <td>NA</td><td>256</td><td>Good</td><td>NA</td></tr>\n",
        "  <tr><td>50000 (TOY)</td><td>300</td><td>NA</td>\n",
        "    <td>NA</td><td>300</td><td>Good</td><td>NA</td></tr>\n",
        "  <tr> <td>100000</td><td>500</td><td>0.4</td><td>NA</td>\n",
        "    <td>500</td><td>Fine</td><td>NA</td></tr>\n",
        "  <tr><td>100000</td><td>400</td><td>0.2</td><td>NA</td>\n",
        "    <td>300</td><td>Repetition but OK</td><td>NA</td></tr>\n",
        "  <tr><td>100000</td><td>300</td><td>0.2</td><td>YES</td>\n",
        "    <td>200</td><td>Repetition, POOR</td><td>NA</td></tr>\n",
        "  <tr><td>100000</td><td>200</td><td>0.2</td><td>YES</td>\n",
        "  <td>200</td> <td>IMPROVEMENT </td>\n",
        "    <td>0.29 0.23 0.29.35</td></tr>\n",
        "  <tr><td>100000</td><td>150_75</td><td>0.2</td><td>YES</td>\n",
        "  <td>200</td> <td>REPETITIVE </td>\n",
        "    <td>0.34 0.38 0.43 0.50</td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkYWUTeYc-YI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "21f3d211-562b-4791-f256-d7406f86f18b"
      },
      "source": [
        "t = np.array(['i am cool','soy genial'],dtype='<U275').reshape((1,2))\n",
        "X = encode_sequences(eng_tokenizer, eng_length,t[:,0] )\n",
        "evaluate_model(model, es_tokenizer, X, t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src=[i am cool], target=[soy genial], predicted=[estoy rezando]\n",
            "0.0 0.0 0.0 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVSPPPfzBkt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "115425e1-40c1-4a95-abda-2010825729c1"
      },
      "source": [
        "t = np.array(['how are you','como estas'],dtype='<U275').reshape((1,2))\n",
        "X = encode_sequences(eng_tokenizer, eng_length,t[:,0] )\n",
        "evaluate_model(model, es_tokenizer, X, t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src=[how are you], target=[como estas], predicted=[que estan]\n",
            "0.0 0.0 0.0 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "netT4wVOdWdq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "23384caa-1db6-45f6-f224-50e0cce74183"
      },
      "source": [
        "t = np.array(['how are you brother','como estas hermano'],dtype='<U275').reshape((1,2))\n",
        "X = encode_sequences(eng_tokenizer, eng_length,t[:,0] )\n",
        "evaluate_model(model, es_tokenizer, X, t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src=[how are you brother], target=[como estas hermano], predicted=[como tan como]\n",
            "0.3333333333333333 0.5773502691896257 0.6959050465952276 0.7598356856515925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7maZHoyqxEnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvbhDp0yxEkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6QIkEbFxLkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}